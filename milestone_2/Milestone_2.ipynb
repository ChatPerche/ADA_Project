{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Impact of Agricultural Practices in the World\n",
    "\n",
    "**ADA Project Milestone 2**\n",
    "\n",
    "This notebook consists of our initial Data Analysis of the FAOSTAT dataset on Food an agriculture. We will first study the contents of the data and its strucuture, before restructuring it in order to start our analysis. Also, some research questions initially asked will be answered by the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset initially contained 78 csv files, but some of them were discarded as they will not be useful for our analysis. We have selected 20 CSVs that would help us with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = [x for x in glob('data/**/*.csv') if 'item_groups' not in x and 'population' not in x]\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split those 20 (but we omitted the population for now) csv into different directories, one for each group of csv. Each group corresponds to one category:\n",
    "```.\n",
    "+-- data/\n",
    "|   +-- emissions_agriculture/\n",
    "|      +-- ...\n",
    "|   +-- emissions_land/\n",
    "|      +-- ...\n",
    "|   +-- environment/\n",
    "|      +-- ...\n",
    "|   +-- inputs/\n",
    "|      +-- ...\n",
    "|   +-- population/\n",
    "|      +-- ...\n",
    "|   +-- production/\n",
    "|      +-- ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A.1 Schema consistency\n",
    "---\n",
    "\n",
    "We will first study the schemas of all the csv files we have in order to see if they are consistent or require changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Checking column names across whole dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's scan all the csv files and check their schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaporte/anaconda3/envs/data.analysis/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/rlaporte/anaconda3/envs/data.analysis/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6e2229976c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscanning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The found columns, grouped, are:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repositories/ADA_Project/scanning.py\u001b[0m in \u001b[0;36mscan_columns\u001b[0;34m(files, col_rename, duplicate_cols)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_rename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_rename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduplicate_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repositories/ADA_Project/data_processing.py\u001b[0m in \u001b[0;36mload_dataframe\u001b[0;34m(filename, col_rename, duplicate_cols)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'unit'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gigagrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gigagrams'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Hardcoded hack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data.analysis/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5173\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5174\u001b[0m         ):\n\u001b[0;32m-> 5175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data.analysis/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data.analysis/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data.analysis/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .str accessor with string \"\u001b[0m \u001b[0;34m\"values!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "from scanning import scan_columns\n",
    "\n",
    "all_columns = scan_columns(csv_files)\n",
    "\n",
    "print(\"The found columns, grouped, are:\\n\")\n",
    "for cols, f in all_columns:\n",
    "    print(list(sorted(cols)), f\"Num files {len(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, sometimes the columns `area` and `areacode` are named `country` and `countrycode`, only because some csv files only contain country data, without country groups. We will rename those as to have a unified schema. Also, some files have the `note`, `elementgroup` and `months` columns. We will look into those in subsequent steps as we are now simply checking whether column naming is consistent.\n",
    "\n",
    "In order to obtain a more consistent column naming, we will rename `country` to `area` and `countrycode` to `areacode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename = {'country': 'area', 'countrycode': 'areacode'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_2 = scan_columns(csv_files, column_rename)\n",
    "print(f\"After renaming, we obtain the following columns:\\n\")\n",
    "for cols, f in all_columns_2:\n",
    "    print(list(sorted(cols)), f\"Num files {len(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Checking which columns to drop\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Column labeled \"`note`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a few files that have different schemas. One column that we should look into before continuing is `note`, as it is in 4 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import load_dataframe\n",
    "from utils import get_column_unique_values\n",
    "\n",
    "files_with_note = all_columns_2[-1][1]\n",
    "for f in files_with_note:\n",
    "    d = load_dataframe(f, column_rename)\n",
    "    note_values = get_column_unique_values(d, ['note'])\n",
    "    print(f\"Values for \\\"note\\\" in {f} are {note_values.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all values for this column are NaN, so we can safely drop the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Columns \"`yearcode`\" and \"`elementgroup`\"\n",
    "\n",
    "We figured it would be useful to scan for duplicate columns in each dataframe (i.e. columns with different names but same values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanning import scan_column_duplicates\n",
    "duplicates = scan_column_duplicates(csv_files, column_rename)\n",
    "for c, f in duplicates:\n",
    "    print(f\"Duplicates for {c} in {len(f)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most files have `year` and `yearcode` columns which are equal, as well as for `elementgroup` and `elementcode`. Hence, we can safely drop `yearcode` and `elementcode` column. \n",
    "\n",
    "Hence, we can define a list of columns to be checked and dropped if they are duplicate of others, and drop all columns who are all `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cols = [\"yearcode\", \"elementgroup\"]\n",
    "all_columns_3 = scan_columns(csv_files, column_rename, duplicate_cols)\n",
    "print(f\"After renaming and dropping columns, we obtain the following columns:\\n\")\n",
    "for cols, f in all_columns_3:\n",
    "    print(list(sorted(cols)), f\"Num files {len(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 15 files with identical schemas, and 2 files that have a different one:\n",
    " - The file containing the `elementgroup` additional column, which refers to groups of `elements`\n",
    " - The file with monthly data and no `item` and `itemcode` columns\n",
    " \n",
    "To obtain the desired format, we can now call `load_clean_dataframe(<file>, column_rename, duplicate_cols)` with `column_rename = {'country': 'area', 'countrycode': 'areacode'}` and `drop_columns = [\"note\", \"yearcode\", \"elementgroup\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only remaining csv with different schema are:\n",
    "- `data/environment/Environment_Temperature_change_E_All_Data_(Normalized).csv` (which contains monthly data)\n",
    "- `data/environment/Environment_Energy_E_All_Data.csv`, which contains `elementgroup` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A.2 Schema description\n",
    "\n",
    "---\n",
    "Now that we have a unified schema for (almost) all csv files, we can start looking into the meaning of each column and their possible value. \n",
    "\n",
    "- `area` and `areacode` : These two fields describe where the measurement was done. Usually, each the relationship should be one-to-one. FAO provides us with a csv file containing the unique one-to-one (two way) mapping, so we can discard the column `area` and only work with `areacodes`\n",
    "- `elementcode`, `element` and `unit`: And element code uniquely identifies an element (i.e. a measure like Production) and a unit (tonnes, heads ...) pair. Since we don't have a mapping provided by FAO, we will construct one with the data. We expect to have a one-to-one mapping between elementcode <-> (element, unit)\n",
    "-  `flag`: There are flags set by the FAO that describe how the data was obtained, for each datapoint. We have decided to drop this column as it is not useful for our analysis.\n",
    "- `item` and `itemcode`: Again, we expect a one-to-one mapping between those two. They represent the category on which the measurement (i.e. (element, unit)) was done. It can be for example \"Cattle\", \"Chickens\", etc ... The mapping will be constructed from the available data.\n",
    "- `value`: This is the actual value of the measure quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Area columns\n",
    "---\n",
    "As said previously, the `area` column will be dropped, but we still need the mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import load_dataframe\n",
    "from mappings import get_mapping, is_unique_mapping\n",
    "area_codes = load_dataframe('data/countries.csv')\n",
    "area_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect regarding those columns. If we look at the second line of the printed dataframe, we can see \"Africa\". FAO provides also another csv that contains country groups, where they describe which country is in which group. This can be useful as to conduct a broader anaylsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groups = load_dataframe('data/country_groups.csv')\n",
    "country_groups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, countries are grouped into multiple `countrygroup`, so we know exactly of which countries each group is formed. These country groups are present in the dataset as `area`, meaning there are aggregated values in the dataset. For example: we can find the emissions for \"Algeria\" and for \"Africa\", where the latter is an aggregated value over the whole group. We will need to be careful when aggregating values in the future, as we could account multiple times for one country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_unique_mapping(area_codes[['country', 'countrycode']].drop_duplicates(), 'country', ['countrycode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Element Columns\n",
    "---\n",
    "\n",
    "The `element` and `elementcode` represent the measure quantity for a given `item`. A quantity has a name and a unit, which is why we believe these two columns should also have a one-to-one mapping accross the whole dataset. Also, since an `elementcode` potentially uniquely identifies (`element`, `unit`) pair, we might drop those two columns as to make the csv files smaller and easier to manipulate.\n",
    "\n",
    "First let's check if indeed this mapping is one-to-one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import get_all_column_unique_values\n",
    "element_values = get_all_column_unique_values(csv_files, column_rename, duplicate_cols, ['elementcode', 'element', 'unit'])\n",
    "is_unique_mapping(element_values, 'elementcode', ['element', 'unit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `elementcode` uniquely identify (`element`, `unit`) pairs, so we can safely drop those two columns and only use `elementcode`. We will later pivot each csv as to obtain all the `elementcode`s as columns, so we can reduce de number of rows significantly. A mapping using a dictionnary will of course be necessary in order to have a nice GUI where users can select the (element, unit) pair instead of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Item columns\n",
    "---\n",
    "\n",
    "According to FAOSTAT, the `item` and `itemcode` columns represent item on which measurements were done. For example an item can be `cattle` and the measurement can be \"CH4 emissions in gigagrams\". \n",
    "Similarly to what we did above, we expect `item` and `itemcode` to have a one-to-one relationship. Let's verify this using the same functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_values = get_all_column_unique_values(csv_files, column_rename, duplicate_cols, ['item', 'itemcode'], with_file=True)\n",
    "is_unique_mapping(item_values[['item', 'itemcode']], 'item', ['itemcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `item` to `itemcode` is not unique for a few items, let's check those and try to understand why it is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd = item_values.groupby('itemcode')['item'].agg(set)\n",
    "grpd[grpd.apply(len) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping `itemcode` -> `item` is unique, now let's check the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = item_values.groupby('item')['itemcode'].agg(set)\n",
    "grouped[grouped.apply(len) >1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Some items are related to pesticides. Let's in which csv files they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesticide_items = [\"Disinfectants\", \"Mineral Oils\", \"Other Pesticides nes\", \"Plant Growth Regulators\"]\n",
    "item_values[item_values.item.isin(pesticide_items)].file.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some duplicate items with the same name but different codes appear in only one csv file, which seems quite odd. We suspect having duplicate rows in that case. Let's check that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import get_duplicate_items\n",
    "t = load_dataframe('data/inputs/Inputs_Pesticides_Use_E_All_Data_(Normalized).csv', column_rename, duplicate_cols)\n",
    "get_duplicate_items(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there are potentially duplicate rows for the same `item` but a different `itemcode` (with the same measurements). Hence, we need to add a functionnality that checks for duplicated items in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Now let's look at livestock items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livestock_items = [\"Cattle\", \"Chickens\"]\n",
    "for i in item_values[item_values.item.isin(livestock_items)].groupby(['item', 'itemcode']).agg(set).reset_index().values:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `itemcode` -> `item` seems one-to-one in each csv file, but we see that there are two distinct `itemcode` for \"Cattle\" and \"Chickens\" accross all csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that different values for `itemcode` for both \"Chickens\" and \"Cattles\" differ when the measured quantity is realted to emissions. Thus, this might be a discrepancy in the data as it does not appear for other items. We will treat this discrepancy once all csv have been merged into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A.4 Grouping items into itemgroups\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a good idea on how the data is structured, it would be good to start thinking how we would perform our analysis to answer our research questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = item_values[['item', 'itemcode']].drop_duplicates().shape[0]\n",
    "print(f\"There are {num_items} different items in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number seems quite large, and filtering/choosing the best items for the analysis seems like a tedious task. Luckily, FAO provides us with `itemgroups`, which are simply groups of items. Those are especially important for production, as they group similar products together.\n",
    "\n",
    "In order to reduce the number of items and reduce the risk of arbitrarily choosing some for the analyis, we will instead work with itemgroups.\n",
    "\n",
    "From the original dataset, we have produced a new one, aggregating items into itemgroups. However, the code is not included in the notebook as it is not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A.3 Conclusion about Schemas\n",
    "\n",
    "---\n",
    "\n",
    "After having applied some analysis on the initially different schemas of the dataframes, we are now able to obtain identitcal schemas all of them but one (which is monthly data).\n",
    "\n",
    "The common schema has the following columns:\n",
    "\n",
    "`['area', 'areacode', 'element', 'elementcode', 'flag', 'item', 'itemcode', 'unit', 'value', 'year']`\n",
    "\n",
    "For now, we have come up with a few steps that should be applied when loading a dataframe:\n",
    " - Rename columns to remove spaces and captial letters\n",
    " - Rename \"Country\" to \"Area\" and \"CountryCode\" to \"Areacode\"\n",
    " - Drop columns that are either duplicates or NaN values\n",
    " - Rename \"gigagrams\" to \"Gigagrams\", as otherwise we would have a non one-to-one mapping for `elementcode`\n",
    " - Check for items if they have duplicate rows.\n",
    "\n",
    "The function that does all of steps is `data_processing/load_clean_dataframe`.\n",
    "\n",
    "Also, we have decided to only work with codes (i.e. `areacode` and `elementcode`) and drop `area`, `element` and `unit`. Of course, we will need a mapping from `code` -> `value`. Additionally, in order to keep track of `countrygroup` and do know how values are aggregated, we will also create a mapping that tells us which countries form a group, by codes.\n",
    "\n",
    "Hence, to load a dataframe, we simply call :\n",
    "```\n",
    "df = load_clean_dataframe(<filename>, column_rename, check_columns, drop_columns)\n",
    "```\n",
    "\n",
    "with  `column_rename = {'country': 'area', 'countrycode': 'areacode'}` and `check_columns = [\"note\", \"yearcode\", \"elementgroup\"]` and `drop_columns = [\"area\", \"item\", \"element\", \"unit\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename = {'country': 'area', 'countrycode': 'areacode'}\n",
    "duplicate_cols = [\"yearcode\", \"elementgroup\"]\n",
    "csv_files = glob(\"data_cleaned/**/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have re-oganized the dataset as to obtain only grouped items. this has drastically reduced the number of items we work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_values = get_all_column_unique_values(csv_files, column_rename, duplicate_cols, ['item', 'itemcode'])\n",
    "element_values = get_all_column_unique_values(csv_files, column_rename, duplicate_cols, ['elementcode', 'element', 'unit'])\n",
    "is_unique_mapping(item_values[['item', 'itemcode']], 'item', ['itemcode']), item_values[['itemcode', 'item']].drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `itemcode`<->`item` mapping is unique, and we have reduced the number of items to 96, which seems much better to work with.\n",
    "\n",
    "\n",
    "Now we need to get the mappings for :\n",
    "- `elementcode` -> (`element`, `unit`) (unique in both ways)\n",
    "- `itemcode` -> `item` (unique in that way, but not the other way around)\n",
    "- `areacode` -> `area` (unique in both ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings import get_mapping, get_area_mapping, get_country_groups\n",
    "item_mapping = get_mapping(item_values,['itemcode','item'])\n",
    "element_mapping = get_mapping(element_values, ['elementcode', 'element', 'unit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_mapping = get_area_mapping(\"data/countries.csv\")\n",
    "country_groups = get_country_groups(\"data/country_groups.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## B. Diving into the data\n",
    "\n",
    "Now that the data is in an appropriate format, we can start our analysis and start following our research Path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. What is the overall impact of the agriculture industry on global green house gas (GHG) emissions ?\n",
    "---\n",
    "\n",
    "This first question that naturally comes to mind. FAO provides data regarding emissions from multiple sectors. Green house gase emissions are computed using `CO2eq` (i.e. CO2 equivalent) from multiple gases (CO2, CH4, N20, and F-gases), which are in top list of Green House Gases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = load_dataframe(\"data_cleaned/environment/Environment_Emissions_by_Sector.csv\").drop(\"unnamed:0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see the share of each sector as a timeline, for the whole world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8)) \n",
    "emission_shares_world = emissions_df[(emissions_df.elementcode == 7263) & (emissions_df.areacode == 5000)]\n",
    "g = sns.lineplot(x='year', y='value', data=emission_shares_world, hue='item', ax=ax)\n",
    "g.legend(loc='center', bbox_to_anchor=(0.5, -0.2), ncol=2);\n",
    "ax.set_title(\"Share of total emissions per sector\")\n",
    "ax.set_ylabel(\"% Of global emissions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that agriculture has stayed around 10% of global emissions by sector for the years 1990-2010. Now we shall look at the share of this emissions for each gas, only looking at the World wide Agriculture emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_pie\n",
    "emission_ghg_shares = emissions_df[emissions_df.elementcode.isin([7264, 7265, 7266]) & (emissions_df.areacode == 5000)]\n",
    "fig, axs = plt.subplots(figsize=(20, 20), ncols=3)\n",
    "for i, code in enumerate(emission_ghg_shares.elementcode.unique()):\n",
    "    ax = axs[i]\n",
    "    df = emission_ghg_shares[emission_ghg_shares.elementcode == code].groupby(['area', 'item'])['value'].mean().reset_index()\n",
    "    labels, vals = df.item.values, df.value.values\n",
    "    title = f\"{element_mapping[code][0]}\"\n",
    "    plot_pie(vals, labels, ax, title)\n",
    "plt.subplots_adjust(wspace = 1.2)\n",
    "plt.legend(labels, loc='center', bbox_to_anchor=(-1.8, -0.7), ncol=2);\n",
    "fig.suptitle(\"Average shares of GHG emissions by sector (1990-2000 period)\", x=0.5, y=0.68, size=15);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this pie chart, we can learn a few things for the Agriculture industry:\n",
    " - It seems that direct CO2 emissions from Agriculture are absent (0%). This makes sense as C02 is not directly emitted. However, the CO2 emissions might be indirect, as food is always transported via non carbon-free methods, and industrial machinery is used in agriculture, and requires energy.\n",
    " - Global CH4 Emissions are shared between Agriculture and Energy industries, with a majority for Agriculture (41.4%)\n",
    " - Global N2O are largely dominated by the Agriculture industry (70%)\n",
    "\n",
    "All of these gases are considered top level Green House gases. It makes sense now to focus on searching where those emissions come from in detail (only for agriculture), and how can we reduce them without comprimising the diet of the population.\n",
    "\n",
    "Now we know which GHG are largely emitted by the agriculture industry, and that Agriculture contributes to roughly 10% of global Emissions, but largely dominates other sectors in terms of CH4 and N2O emissions. \n",
    "\n",
    "However, this plot does not take into consideration indirect emissions due to Agriculture:\n",
    "- Emissions due to transport of food (included in \"Transport\" sector)\n",
    "- Emissions due to use of energy for producing agricultural products (included in \"Energy\")\n",
    "\n",
    "Our analysis focuses more on the direct emissions of Agriculture, and hence we will not take those aspects in consideration as they fall outside of the scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. What are the sources of CH4 and N2O emissions from in the Agriculture Industry ?\n",
    "---\n",
    "\n",
    "To illustrate this, we can separate the emissions of both gases into 10 items related to agriculture: Enteric Fermentation, Manure management, Rice Cultivation, Synthetic Fertilizers, Manure applied to soils, Manure left on Pasture, Crop residues, Burning of crop residues, Burning of savanna and Cultivation of organic Soils.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_countries = country_groups[5000]\n",
    "emissions_agriculture = load_dataframe(\"data_cleaned/emissions_agriculture/Emissions_Agriculture_Agriculture_total.csv\").drop(\"unnamed:0\", axis=1)\n",
    "emissions_agriculture = emissions_agriculture[emissions_agriculture.year < 2020] # Filter predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import add_zero_values\n",
    "co2_ch4_emissions_codes = [7225, 7230]\n",
    "\n",
    "# Get elements of interest, and world countries\n",
    "emissions_agr_ch4_co2_world = emissions_agriculture[emissions_agriculture.elementcode.isin(co2_ch4_emissions_codes) & emissions_agriculture.areacode.isin(world_countries)]\n",
    "emissions_agr_ch4_co2_world = emissions_agr_ch4_co2_world.groupby(['elementcode', 'element', 'unit', 'item', 'itemcode', 'year'])['value'].sum().reset_index()\n",
    "# Group by to get world values\n",
    "emissions_agr_ch4_co2_world_grpd = emissions_agr_ch4_co2_world.groupby(['elementcode', 'element', 'unit', 'item', 'itemcode'])['value'].mean().reset_index()\n",
    "emissions_agr_ch4_co2_world_grpd = add_zero_values(emissions_agr_ch4_co2_world_grpd, element_mapping, item_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_pie\n",
    "fig, axs = plt.subplots(figsize=(15, 10), ncols=2)\n",
    "for i, code in enumerate(emissions_agr_ch4_co2_world_grpd.elementcode.unique()):\n",
    "    ax = axs[i]\n",
    "    df = emissions_agr_ch4_co2_world_grpd[emissions_agr_ch4_co2_world_grpd.elementcode == code]\n",
    "    labels, vals = df.item.values, df.value.values\n",
    "    title = f\"{element_mapping[code][0]}\"\n",
    "    plot_pie(vals, labels, ax, title)\n",
    "plt.subplots_adjust(wspace = 1.2)\n",
    "plt.legend(labels, loc='upper right', bbox_to_anchor=(2.2, 0.8), ncol=1);\n",
    "fig.suptitle(\"Average yearly shares of CH4 and N2O emissions for agriculture (1961-2017 period)\", x=0.5, y=0.9, size=15);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart illustrates quite well the origin of emissions of CH4 and N2O, in the agriculture industry. Before interpreting the chart, let's brievly explain what each item represents:\n",
    "- Burning - Crop residues: Agricultural practice that consists of combusting a percentage of crop residues on site\n",
    "- Burning - Savanna: Periodic prescribed burning of savanna for agricultural purposes\n",
    "- Enteric Fermentation: Digestive process by which carbohydrates are broken down by micro organisms into simple molecules for absorption into the bloodstream of an animal.\n",
    "- Manure Management: Refers to capture, storage, treatment, and utilization of animal manure\n",
    "- Rice Cultivation: Agricultural practice for growing rice seeds\n",
    "- Crop Resdiues: Agriculture management practice that consists in returning to managed soils the residual part of the produce\n",
    "- Cultivation of Organic Soils:\n",
    "- Manure applied to Soils: Animal waste distributed on fields in amounts that enrich soils\n",
    "- Manure left on pasture: Animal waste left on managed soils from grazing livestock. \n",
    "- Synthetic Fertilizers: Inorganic material of synthetic origin added to a soil to supply one or more plant nutrients essential to the growth of plants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chart Intepretation:\n",
    "1. CH4 Emissions:\n",
    "\n",
    "    - Direct CH4 emissions of agriculture are mostly dominated by the Enteric Fermentation of Livestock (69.4%), which means that this particular source of emissions accounts for rougly 30% of the global CH4 Emissions, which is quite significant.\n",
    "    - Rice Cultivation also is a big contributor (18%) to the emissions of this GHG.\n",
    "    - Other sources have much lower contribution to emissions of this gas\n",
    "\n",
    "2. N2O Emissions:\n",
    "    - The biggest contributor is \"Manure Left on Pasture\", follows by Synthetic Fertilizers.\n",
    "    - The use of Manure (Manure management, left on pasture, and applied to soils) have a contribution of roughly 40% (together) of N2O Emissions for agriculture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. How has the emissions global emissions of those gases from each source evolved in the past years ?\n",
    "---\n",
    "Having an average share of each source over the past 50 years gives us a general idea of where the emissions come from. Additionally, the evolution of emissions in this time frame is also interesting.\n",
    "\n",
    "We all know Global emissions have increased worldwide, but what are the sources of this increase in the agricultural sector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. General evolution of emissions of CH4 and N2O in agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15, 5), ncols=2)\n",
    "plot_data = emissions_agr_ch4_co2_world.groupby(['elementcode', 'element', 'unit', 'year'])['value'].sum().reset_index()\n",
    "for i, e in enumerate(plot_data.elementcode.unique()):\n",
    "    ax = axs[i]\n",
    "    sns.lineplot(x='year', y='value', data=plot_data[plot_data.elementcode == e], hue='element', ax=ax)\n",
    "    ax.set_ylabel(\"Gigagrams\")\n",
    "    ax.set_title(f\"Evolution of {element_mapping[e][0]} in Agriculture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice a clear linear increase in the emission of both gases , with a slight peak in 1990-1995. Now let's look at the decomposition into all the sources we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evolution of emissions of CH4 and N2O in agriculture for each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 8), ncols=2) \n",
    "for i, e in enumerate(emissions_agr_ch4_co2_world.elementcode.unique()):\n",
    "    ax = axs[i]\n",
    "    g = sns.lineplot(x='year', y='value', data=emissions_agr_ch4_co2_world[emissions_agr_ch4_co2_world.elementcode == e], hue='item', ax=ax)\n",
    "    g.legend(loc='center', bbox_to_anchor=(0.5, -0.2), ncol=2);\n",
    "    elem, unit = element_mapping[e]\n",
    "    ax.set_title(f\"World wide {elem} for different sources\")\n",
    "    ax.set_ylabel(f\"{unit}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Analysis:\n",
    "1. Emissions of CH4:\n",
    "As seen previously, we knew that CH4 emissions are dominated by the Enteric Fermentation, and we can see that it has increased by roughly 50% since 1961. However other sources do not seem to have had the same increase in emissions.\n",
    "\n",
    "2. Emissions of N2O:\n",
    "Increases here are more interesting. We can see that emissions due to Synthetic fertilizers have increased linearly by almost 400%, with slightly lower steepness after ~1995. \n",
    "Manure Management's emissions also increased linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Which countries contribute the most to emissions due to agriculture ?\n",
    "---\n",
    "\n",
    "We now have a global view on emissions worldwide, but it would be interesting to have a more detailed approach and see which countries countribute the most to the emissions of those 2 gases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Raw data\n",
    "First we plot the total emissions in gigagrams for each country and gas (for CH4 and N2O) as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import merge_with_geopandas\n",
    "emissions_agr_by_country = emissions_agriculture[emissions_agriculture.elementcode.isin([7225, 7230])].groupby(['areacode', 'area', 'elementcode', 'element', 'unit', 'year'])['value'].sum().reset_index()\n",
    "emissions_agr_by_country = emissions_agr_by_country.merge(area_codes[['countrycode', 'iso3code']], left_on='areacode', right_on='countrycode').drop('countrycode', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15), nrows=2)\n",
    "plot_map_ch4 = merge_with_geopandas(emissions_agr_by_country[emissions_agr_by_country.elementcode == 7225])\n",
    "plot_map_n2o = merge_with_geopandas(emissions_agr_by_country[emissions_agr_by_country.elementcode == 7230])\n",
    "\n",
    "plot_map_ch4[plot_map_ch4.year == 2017].plot(column='value', cmap=\"OrRd\", ax=ax[0], legend=True, legend_kwds={'label': \"Gigagrams \", 'orientation': \"vertical\"});\n",
    "ax[0].set_title(\"Emissions of CH4 by country in 2017\");\n",
    "\n",
    "plot_map_n2o[plot_map_n2o.year == 2017].plot(column='value', cmap=\"OrRd\", ax=ax[1], legend=True, legend_kwds={'label': \"Gigagrams \", 'orientation': \"vertical\"});\n",
    "ax[1].set_title(\"Emissions of N2O by country in 2017\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 4-5 countries contribute much more than others to emissions of the two gases: China, India, United States, Brazil and Australia. However, these countries' populations are quite signifcant. Let's look at the same graph, but after normalizing by population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalized by population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = load_dataframe(\"data/population/Population_E_All_Data_(Normalized).csv\", column_rename, duplicate_cols)\n",
    "population_df = population_df[(population_df.elementcode == 511)][['areacode', 'year', 'value']]\n",
    "population_df['value'] = population_df['value'] * 1000 # Unit is in 1000 persons, we want the real population\n",
    "population_df = population_df.rename(columns={'value': 'population'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_by_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15), nrows=2)\n",
    "emissions_agr_by_country_with_pop = normalize_by_pop(emissions_agr_by_country, population_df)\n",
    "\n",
    "plot_map_ch4 = merge_with_geopandas(emissions_agr_by_country_with_pop[emissions_agr_by_country_with_pop.elementcode == 7225])\n",
    "plot_map_ch4 = plot_map_ch4[plot_map_ch4.year == 2017]\n",
    "plot_map_n2o = merge_with_geopandas(emissions_agr_by_country_with_pop[emissions_agr_by_country_with_pop.elementcode == 7230])\n",
    "plot_map_n2o = plot_map_n2o[plot_map_n2o.year == 2017]\n",
    "\n",
    "plot_map_ch4.plot(column='value', cmap=\"OrRd\", ax=ax[0], legend=True, legend_kwds={'label': \"Gigagrams/capita\", 'orientation': \"vertical\"});\n",
    "ax[0].set_title(\"Emissions per capita of CH4 by country in 2017\");\n",
    "\n",
    "plot_map_n2o.plot(column='value', cmap=\"OrRd\", ax=ax[1], legend=True, legend_kwds={'label': \"Gigagrams/capita\", 'orientation': \"vertical\"});\n",
    "ax[1].set_title(\"Emissions per capita of N2O by country in 2017\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, it seems that all countries have similar values of emission gigagrams/capita, but the scale tells us that some countries have higher values. Let's plot the distribution of values and check how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5), ncols=2)\n",
    "sns.distplot(plot_map_ch4['value'], ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of CH4 emissions gigagrams/capita\")\n",
    "sns.distplot(plot_map_n2o['value'], ax=ax[1])\n",
    "ax[1].set_title(\"Distribution of N2O emissions gigagrams/capita\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem to be very concentrated around 0, but both distributions have a very long tail. This could be due to a faulty value for one or a couple of countries.\n",
    "\n",
    "After further inspection, we saw that one country seems to stand out and has a much higher value: Falkland Islands, a small group of islands in the southern Atlantic, with a population of around 3000 people. We believe this data is clearly wrong, as this country's agriculture is not an important economy. It either comes from mis-interpolated data by the FAO, or simply an outlier. For now, we will drop it and redo the plots to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15), nrows=2)\n",
    "plot_map_ch4[plot_map_ch4.areacode != 65].plot(column='value', cmap=\"OrRd\", ax=ax[0], legend=True, legend_kwds={'label': \"Gigagrams/capita\", 'orientation': \"vertical\"});\n",
    "ax[0].set_title(\"Emissions per capita of CH4 by country in 2017\");\n",
    "\n",
    "plot_map_n2o[plot_map_n2o.areacode != 65].plot(column='value', cmap=\"OrRd\", ax=ax[1], legend=True, legend_kwds={'label': \"Gigagrams/capita\", 'orientation': \"vertical\"});\n",
    "ax[1].set_title(\"Emissions per capita of N2O by country in 2017\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map looks much better now. We can see which countries stand out: Australia and Mongolia seem to have a very high emission factor per capita for both gases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Which crops/livestock production has increased during that time frame ? And is the increase linear with the increase of population ?\n",
    "---\n",
    "\n",
    "We will now shift our focus to production of various crops and livestock. The data is separated in 3 categories: Livestock (live animals), Livestock produce (meat, eggs, etc..) and Crops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evolution of Livestock stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_livestock = load_dataframe(\"data_cleaned/production/Production_Livestock_All_data.csv\").drop(\"unnamed:0\", axis=1)\n",
    "production_livestock_world = production_livestock[production_livestock.areacode.isin(world_countries)]\\\n",
    "                                .groupby(['elementcode', 'element', 'unit', 'itemcode', 'item', 'year'])['value'].sum().reset_index()\n",
    "production_livestock_world = production_livestock_world[production_livestock_world.itemcode != 1756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7), ncols = production_livestock_world.elementcode.nunique())\n",
    "for i, e in enumerate(production_livestock_world.elementcode.unique()):\n",
    "    df = production_livestock_world[production_livestock_world.elementcode == e]\n",
    "    sns.lineplot(x='year', y='value', data=df, hue='item', ax=ax[i])\n",
    "    ax[i].set_title(\"Stock of Live animals worldwide\")\n",
    "    ax[i].set_ylabel(element_mapping[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, of course, all stocks of live animals have increased. These take into consideration all animals used to produce food: it can be dairy cattle, layer chicken etc.. \n",
    "\n",
    "Now we normalize by population to see if the increase is proportional to the increase in population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_livestock_world['areacode'] = 5000\n",
    "production_livestock_world_norm = normalize_by_pop(production_livestock_world, population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7), ncols = production_livestock_world_norm.elementcode.nunique())\n",
    "for i, e in enumerate(production_livestock_world_norm.elementcode.unique()):\n",
    "    df = production_livestock_world_norm[production_livestock_world_norm.elementcode == e]\n",
    "    sns.lineplot(x='year', y='value', data=df, hue='item', ax=ax[i])\n",
    "    ax[i].set_title(\"Stock of Live animals worldwide per capita\")\n",
    "    ax[i].set_ylabel(f\"{element_mapping[e][1]} / capita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the amount of \"Sheep and Goats\" and \"Cattle and Buffaloes\" per capita seems to be decreasing, while for Poultry birds it is clearly increasing. \n",
    "This could be due to multiple factors:\n",
    "- Cattle, Sheep and Goats need more time to grow, enducing a higher cost in production\n",
    "- Poultry Birds are cheaper to feed, need less space to grow, and we have developped techniques where they can be stacked by millions easily, which is not the case for bigger animals.\n",
    "- The increase in population was more dominant in countries where Red meat consumption is lower (India).\n",
    "- People prefer to eat chicken today rather than in the 60s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evolution of production of Livestock products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_livestock_prim = load_dataframe(\"data_cleaned/production/Production_Livestock_Primary_All_data.csv\")\n",
    "production_livestock_prim = production_livestock_prim[production_livestock_prim.areacode.isin(world_countries)]\\\n",
    "                                .groupby(['elementcode', 'element', 'unit', 'itemcode', 'item', 'year'])['value'].sum().reset_index()\n",
    "production_livestock_prim = production_livestock_prim[(~production_livestock_prim.itemcode.isin([1765])) & (production_livestock_prim.elementcode == 5510)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7), ncols = production_livestock_prim.elementcode.nunique())\n",
    "for i, e in enumerate(production_livestock_prim.elementcode.unique()):\n",
    "    df = production_livestock_prim[production_livestock_prim.elementcode == e]\n",
    "    sns.lineplot(x='year', y='value', data=df, hue='item', ax=ax)\n",
    "    ax.set_title(\"Production of Livestock Produce\")\n",
    "    ax.set_ylabel(element_mapping[e][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_livestock_prim['areacode'] = 5000\n",
    "production_livestock_prim = normalize_by_pop(production_livestock_prim, population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7), ncols = production_livestock_prim.elementcode.nunique())\n",
    "for i, e in enumerate(production_livestock_prim.elementcode.unique()):\n",
    "    df = production_livestock_prim[production_livestock_prim.elementcode == e]\n",
    "    sns.lineplot(x='year', y='value', data=df, hue='item', ax=ax)\n",
    "    ax.set_title(\"Production of Livestock Produce per capita\")\n",
    "    ax.set_ylabel(f\"{element_mapping[e][1]} / capita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evolution of production of crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_crops = load_dataframe(\"data_cleaned/production/Production_Crops_All_data.csv\")\n",
    "production_crops = production_crops[production_crops.areacode.isin(world_countries)]\\\n",
    "                                .groupby(['elementcode', 'element', 'unit', 'itemcode', 'item', 'year'])['value'].sum().reset_index()\n",
    "production_crops = production_crops[(~production_crops.itemcode.isin([1717, 1714]))&(production_crops.elementcode == 5510)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_crops[['itemcode','item' ]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7), ncols = production_crops.elementcode.nunique())\n",
    "for i, e in enumerate(production_crops.elementcode.unique()):\n",
    "    df = production_crops[production_crops.elementcode == e]\n",
    "    sns.lineplot(x='year', y='value', data=df, hue='item', ax=ax)\n",
    "    ax.set_title(\"Production of Crops\")\n",
    "    ax.set_ylabel(element_mapping[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Nutritional Values\n",
    "Here we will investigate different nutritional values of food, as well as requirements for a healthy diet and which crops / meat contain which nutrients.\n",
    "This is only a summary of what is done in the nutrient_exploration notebook.\n",
    "\n",
    "### Diet diversity\n",
    "Nutrients are contained in different quantities in different commodities; for example:\n",
    "- roots and tubers contain a lot of vitamin A but a low amount of protein\n",
    "- livestock and fish contain a lot of protein and vitamin B12\n",
    "- some nutrients are generally harder to come by than others\n",
    "\n",
    "Therefore a healthy diet requires food intake diversity.\n",
    "\n",
    "Diversity is also important from an agricultural point of view; different types of crops or different species of a given crop will have less chance of all being wiped out by a single disease. \n",
    "\n",
    "### Malnutrition\n",
    "\n",
    "The first cause of death by malnutrition is caused by a lack of protein. The most common mineral deficiency is iron deficiency.\n",
    "\n",
    "\n",
    "Different regions have different diets and different deficiencies. In the USA for example, the nutrients that are under-consumed are fiber, calcium, vitamin D and potassium. However, in less-developped countries, there are a lot of iron and protein deficiencies.\n",
    "\n",
    "\n",
    "## Creating a nutrient list\n",
    "\n",
    "These nutrients are deemed to be essential not only because of their paramount importance to a healthy diet but also because of the fact that there exists widespread deficiencies worldwide.\n",
    "\n",
    "Some of these can be easily manufactured and distributed (e.g. iodine). We will only focus on a subset of nutrients, and look at protein content, fiber content, lipid content, vitamin content and mineral content of given foods in order to construct a \"healthy\" diet (meaning one that provides good nutrient intake).\n",
    "### What we will take into consideration for now:\n",
    "\n",
    "- Proteins\n",
    "- Carbohydrates\n",
    "- Fibers\n",
    "- Lipids\n",
    "- Vitamins and minerals\n",
    "    - Vitamins: A, B6, B9, B12, C, D, E\n",
    "    - Minerals : Fe, Ca\n",
    "    \n",
    "    \n",
    "    \n",
    "# Nutrient value database\n",
    "In parallel with the evaluation of the environmental impact of livestock and crops, we need to decide which food yield the best (or at least a good/healthy enough) nutritive value. \n",
    "To do this, after having decided on certain categories of nutrients (vitamins / fibers / etc...), we will check that the nutrients that we plan on using contain enough nutritive value using this database:\n",
    "https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/food-surveys-research-group/docs/fndds-download-databases/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going Further** \n",
    "\n",
    " - When and where fertilizer/pesticide use started being used globally and what were their impact on production/emissions?\n",
    " - Which fertilizers are the most efficient in terms of production/emissions?\n",
    " - Which fertilizers brought the best increase in production ?\n",
    " \n",
    "The above questions reduce to looking at the impact of a multivariate time series on other time series, quantifing that impact, looking at whether or not certain factors far outweigh others, and identifying those if any such exist. \n",
    "\n",
    "In order to do so, we first need to establish how fertilizers and pesticides impact different agricultural productions and subsequent emissions. Several methods could be used, but in order to preserve the dimension of time in our analysis, using a stacked multivariate LSTM seemed most appropriate ( see Understanding LSTM - https://arxiv.org/abs/1909.09586). Even if the surrounding litterature would suggest that LSTMs could be outperformed by other deep models, we wished to more formally emphasize the nature of our data as a time series in our anlaysis. \n",
    "\n",
    "The desire to infer certain variables from another in our dataset does not come from a desire to develop any predictions, but rather to check that we have a sound way of interpreting how our variables impact eachother. Initial tests, done without compensating for the heavy correlations and potentially skewed distributions of our data points, show promising results. The  Once we have a sufficiently accurate tool, we can then use a SHAP Kernel Explainer (see A Unified Approach to Interpreting Model Predictions - https://arxiv.org/abs/1705.07874 ), which will quantify how each input time series impacts the model output. In our case, the advantage of this method (based on Shapley values) over a more traditional analysis of gradients and derivatives is it's ability to compensate for missing data in our time series without biasing the impact of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data.analysis",
   "language": "python",
   "name": "data.analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
