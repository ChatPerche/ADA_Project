{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import *\n",
    "from mappings import *\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import pandas as pdk\n",
    "import glob\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "assert(tf.test.is_gpu_available()   and\\\n",
    "       tf.test.is_built_with_cuda() and\\\n",
    "       tf.test.is_built_with_gpu_support())\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files     = glob.glob('data/**/**.csv')\n",
    "column_rename = {'country': 'area', 'countrycode': 'areacode'}\n",
    "check_columns = [\"note\", \"yearcode\", \"elementgroup\"]\n",
    "\n",
    "from mappings import get_mapping, get_area_mapping, extract_element_to_item\n",
    "\n",
    "item_mapping    = get_mapping(csv_files, column_rename, check_columns, ['itemcode','item'])\n",
    "element_mapping = get_mapping(csv_files, column_rename, check_columns, ['elementcode', 'element', 'unit'])\n",
    "df              = pd.read_csv(\"df.csv\")\n",
    "element_to_item = extract_element_to_item(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "print(df[~df['5510'].isna()].itemcode.value_counts())\n",
    "#print([(e,element_mapping[int(e)]) for e,_ in element_to_item.items()])\n",
    "print((7231,element_mapping[7231]))\n",
    "print([(i,item_mapping[i]) for i in element_to_item[str(7231)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([(e,element_mapping[int(e)]) for e,_ in element_to_item.items()])\n",
    "print((5510,element_mapping[5510]))\n",
    "print([(i,item_mapping[i]) for i in element_to_item[str(5510)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training_samples:  4512\n"
     ]
    }
   ],
   "source": [
    "input_pairs   = [(5510,item_code) for item_code in (1765,1808,1738,1058,1062)]\n",
    "output_pairs  = [(7231,1711)]\n",
    "samples       = build_samples(df,20,input_pairs,output_pairs)\n",
    "\n",
    "non_differentiated  = df[df.itemcode == 1711][['areacode','year',str(7231)]].set_index(['areacode','year'])\n",
    "differentiated      = introduce_differentiation(non_differentialed)\n",
    "adjustment_ratio          = ((differentiated.max()-differentiated.min())).values[0]\n",
    "#avg_percentage_adjustment = differentiated.mean().values[0]\n",
    "\n",
    "def adj_rms(y_true, y_pred): \n",
    "    return tf.sqrt(tf.reduce_mean(((y_true - y_pred)*(adjustment_ratio))**2))\n",
    "\n",
    "\n",
    "train_X, train_Y, test_X, test_Y  = reshuffle_observations(samples,0.80,len(output_pairs))\n",
    "print(\"Number of training_samples: \",len(train_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46 steps, validate on 12 steps\n",
      "Epoch 1/1000\n",
      "46/46 [==============================] - 6s 129ms/step - loss: 0.0356 - adj_rms: 0.3318 - val_loss: 0.0297 - val_adj_rms: 0.3032\n",
      "Epoch 2/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0172 - adj_rms: 0.2236 - val_loss: 0.0039 - val_adj_rms: 0.1079\n",
      "Epoch 3/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0026 - adj_rms: 0.0875 - val_loss: 0.0028 - val_adj_rms: 0.0895\n",
      "Epoch 4/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 5/1000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 6/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 7/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 8/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 9/1000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0025 - adj_rms: 0.0849 - val_loss: 0.0028 - val_adj_rms: 0.0892\n",
      "Epoch 10/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 11/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 12/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0849 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 13/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0850 - val_loss: 0.0027 - val_adj_rms: 0.0888\n",
      "Epoch 14/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 15/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0887\n",
      "Epoch 16/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0889\n",
      "Epoch 17/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0889\n",
      "Epoch 18/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0028 - val_adj_rms: 0.0892\n",
      "Epoch 19/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 20/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 21/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 22/1000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 23/1000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - adj_rms: 0.0848 - val_loss: 0.0027 - val_adj_rms: 0.0886\n",
      "Epoch 24/1000\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 0.0016 - adj_rms: 0.0690"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f34f20ce5e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m history      = model.fit(training_set,validation_data=testing_set,\n\u001b[0;32m---> 23\u001b[0;31m                          epochs=1000,verbose=1, shuffle=False)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#pyplot.plot(history.history['loss'], label='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_samples = train_X.shape[0]\n",
    "\n",
    "model = build_lstm(num_input_timeseries=train_X[0].shape[1],\n",
    "                   num_output_dimensions=train_Y[0].shape[1],\n",
    "                   num_timesteps=train_X[0].shape[0],\n",
    "                   model_complexity=80,\n",
    "                   batch_size=batch_size )\n",
    "\n",
    "\n",
    "model.compile(optimizer=k.optimizers.Adam(lr=0.0001),\n",
    "              loss=k.losses.MSE,metrics=[adj_rms])\n",
    "\n",
    "training_set = tf.data.Dataset.from_tensor_slices((    \\\n",
    "                            tf.cast(train_X, tf.float32), \\\n",
    "                            tf.cast(train_Y, tf.float32))).batch(batch_size).shuffle(num_samples)\n",
    "\n",
    "testing_set = tf.data.Dataset.from_tensor_slices((\n",
    "                            tf.cast(test_X,tf.float32),\n",
    "                            tf.cast(test_Y,tf.float32))).batch(batch_size).shuffle(num_samples)\n",
    "    \n",
    "history      = model.fit(training_set,validation_data=testing_set,\n",
    "                         epochs=1000,verbose=1, shuffle=False)\n",
    "\n",
    "#pyplot.plot(history.history['loss'], label='train')\n",
    "#pyplot.plot(history.history['loss'], label='test')\n",
    "pyplot.plot(history.history['adj_rms'], label='train')\n",
    "pyplot.plot(history.history['adj_rms'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
